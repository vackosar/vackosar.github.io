---
title: "PaLM"
description: "lang model"
layout: post
categories: ml
date: 2022-04-10
permalink: /:categories/:title
---


Pathways Language Model (PaLM) is a 540-billion parameter with architecture akin to GPT-3.
This model, published April 4th, 2022, achieves breakthrough capabilities on language understanding and generation, reasoning, and coding tasks.
For example for reasoning tasks, PaLM used chain-of-thought prompting, which applies simulated inner monologue to solve grade school level math questions.
In this talk, Vaclav will discuss both general-public-accessible intuition of how knowledge and reasoning can be represented in computers as well as technical details of PaLM model architecture.

## Intro
- at the begging there was a word
- in a vocabulary
- counting methods
- gradient methods
- vector methods
    - Tao te ching - every positive tjoight negative - direction and anti direction
- RNNs
- Transfomers
- knowledge graph - what is knowledge
- softmax forces to make a decision, a colapse
- projection and subspaces
- last 230 iq person

## Summary
- 540-billion parameters
- 2x bigger than GPT-3 175B
- 2x smaller than [Switch Transformer](https://arxiv.org/pdf/2101.03961.pdf) 1T
  - only parts of the model is activated at each time.
- but the most [expensive model trained ~$10M](https://blog.heim.xyz/palm-training-cost/)
- human brain 100T connections

- highly efficient training on 6144 TPU v4 chips across multiple cluster TPU Pods
- discontinuous improvements: disproportionate jumps in accuracy from 62B to 540 compared to 8B to 62B
- breakthrough performance without fine-tuning
- breakthrough performance outperforming average human on BIG-bench benchmark
- for reasoning, PaLM uses chain-of-thought prompting, which simulates human reasoning inner monologue applied to grade school level math questions.


## Breakthrough capabilities
- when combined with chain-of-thought prompting
- few-shot evaluation outperforms fine-tuned previous SOTA
- BIG-bench is 150+ tasks
- 58 tasks comparable to other models
- many are multiple choice questions
- PaLM is SOTA outperforming GPT-3 and human average
- still underperforms average human on many tasks
- funny footnote: the dataset is scanned for in the training set using GUID
- certain capabilities of the model only emerge once a certain scale is reached

![discontinuous improvements with scale](/images/palm-discontinuous-improvement-with-scale.png)


## Chain of Thought Prompting
- multi-step arithmetic
  - grade-school level math problems
  - difficult is to convert to equations
- commonsense logical reasoning
  - strong world knowledge
  - chaining logical inferences
- chain of thought prompting teaches reasoning steps
- these are additionally useful for interpretation

![Chain of Thought Prompting](/images/palm-chain-of-though-prompting.png)


## Chain of Thought Prompting Results
- reasoning tasks datasets: GSM8K, SVAMP, MAWPS, AQuA, CommonsenseQA, StrategyQA
- on GSM8K: PaLM+chain-of-thought competitive with GPT-3+finetuning+calculator+verifier 
- 

## Training Dataset
- 780 billion tokens of high-quality text but private
- social media 50%, webpages 27%, books 13%, wikipedia 4%, code 5%, news 1%
- based on dataset used for LaMBDA, GLaM
- private non-reproducible dataset
- MT-NLG used 339B tokens reproducible dataset non-hosted

![PaLM dataset hierarchical topics](/images/PaLM-dataset-hierarchical-topics.png)


## Efficient Scaling
- mind of PalM is shattered across many chips
- ~17 TB of RAM needed for training
- 2 TPU v4 Pod clusters connected data center network
- each Pod 
  - 768 hosts, 3072 TPU v4 chips
  - 1 exaflop/s 
-  gives overview of the parallelism methods

## Training Large Scale Models
No worker has enough compute or memory to host large scale model.
Mainly because it is cheaper to buy many smaller machines: replaceable, easier to cool down.
But that requires us to split the computation to multiple processors, memories with limited communication.
In parallel computing we trade off 3 resources: compute ("time"), memory ("space"), communication throughput (no cool name).
Following strategies are used in large scale model training e.g. [Megatrong-Turing (Microsoft and NVidia)](https://arxiv.org/pdf/2201.11990.pdf): 

- data parallelism
  - batches are divided between works
- pipeline mechanism
  - splits model computation DAG into stages e.g. into layers
  - stages exchange forward and backward propagation information into micro-batches
  - step by step passing causes "bubble" of idle devices
- tensor model parallelism
  - splits model layers i.e. transformer block into parts

![NVIDIA data center DGX AI](/images/nvidia-data-center-dgx-ai.jpg)

## Training Infrastructure
- PaLM uses pipeline-free 2D parallelism
- data parallel across clusters "Pods"
- each cluster has full model copy
  - model partitioned into 12 parts
  - data partitioned into 256 parts
  - 1.5k hosts connected to 3k chips which are interconnected
- update to identical models after each batch
- each host exchanges 1.3GB with its counterpart

![Pathways system datacenter network, tpu ](/images/palm-pathways-system-datacenter-pods-hosts-TPU-chips.png)


## Training Efficiency
- observed throughput relative to the theoretical maximum throughput of a system
- price for parallelization of PaLM is 50%
- in case of PaLM throughput is tokens-per-second

![Model FLOPs utilization of PaLM vs Megatron-Turing NLG vs Gopher vs GPT-3](/images/PaLM-model-flop-utilization-vs-megatron-vs-gopher-gpt-3.png)


## Other notes - TODO cleanup
- blog https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html
- a 540-billion parameter, dense decoder-only Transformer model
- with breakthrough capabilities on language understanding and generation, reasoning, and code-related
- decoder-only Transformer ~ like GPT-3
- https://www.searchenginejournal.com/google-pathways-ai/428864/#close
- Title: 
	Google's Pathways Language Model and Chain-of-Thought
- Text:
	