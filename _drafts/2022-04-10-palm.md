---
title: "PaLM"
description: "lang model"
layout: post
categories: ml
date: 2022-04-10
permalink: /:categories/:title
---

## Summary
- 540-billion parameters
- 2x bigger than GPT-3 175B
- 2x smaller than [Switch Transformer](https://arxiv.org/pdf/2101.03961.pdf) 1T
  - only parts of the model is activated at each time.
- but the most [expensive model trained ~$10M](https://blog.heim.xyz/palm-training-cost/)
- human brain 100T connections

- highly efficient training on 6144 TPU v4 chips across multiple cluster TPU Pods
- breakthrough performance without fine-tuning
- breakthrough performance outperforming average human on BIG-bench benchmark

## Training Dataset
- 780 billion tokens of high-quality text


## Breakthrough capabilities
- when combined with chain-of-thought prompting
- few-shot evaluation outperforms fine-tuned previous SOTA
- BIG-be

## Efficient Scaling
- pipeline-free allows bigger scale
- https://blog.heim.xyz/palm-training-cost/


## My Intro
- at the begging there was a word
- in a vocabulary
- counting methods
- vector methods
- RNNs
- Transfomers
