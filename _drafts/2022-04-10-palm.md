---
title: "PaLM"
description: "lang model"
layout: post
categories: ml
date: 2022-04-10
permalink: /:categories/:title
---

## Summary
- 540-billion parameters
- 2x bigger than GPT-3 175B
- 2x smaller than [Switch Transformer](https://arxiv.org/pdf/2101.03961.pdf) 1T
  - only parts of the model is activated at each time.
- but the most [expensive model trained ~$10M](https://blog.heim.xyz/palm-training-cost/)
- human brain 100T connections

- highly efficient training on 6144 TPU v4 chips across multiple cluster TPU Pods
- breakthrough performance without fine-tuning
- breakthrough performance outperforming average human on BIG-bench benchmark

## Training Dataset
- 780 billion tokens of high-quality text


## Breakthrough capabilities
- when combined with chain-of-thought prompting
- few-shot evaluation outperforms fine-tuned previous SOTA
- BIG-be

## Efficient Scaling
- pipeline-free allows bigger scale
- https://blog.heim.xyz/palm-training-cost/


## My Intro
- at the begging there was a word
- in a vocabulary
- counting methods
- vector methods
- RNNs
- Transfomers

## PaLM
## Chain of Thought results

# Other notes - TODO cleanup
- Add
	- chain of thought - thinking as execution
	- knowledge
		- start with a vectors
		- Tao te ching - every positive tjoight negative - direction and anti direction
		- knowledge graph - what is knowledge
		- softmax forces to make a decision, a colapse
	- projection and subspaces
	- last 230 iq person
- blog https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html
- Inspiration https://www.searchenginejournal.com/google-pathways-ai/428864/#close 
- 540-billion parameter
- a 540-billion parameter, dense decoder-only Transformer model
- with breakthrough capabilities on language understanding and generation, reasoning, and code-related
- for reasoning, PaLM uses chain-of-thought prompting, which simulates human reasoning inner monologue applied to grade school level math questions.
- decoder-only Transformer ~ like GPT-3
- https://www.searchenginejournal.com/google-pathways-ai/428864/#close
- Title: 
	Google's Pathways Language Model and Chain-of-Thought
- Text:
	-  Pathways Language Model (PaLM) is a 540-billion parameter with architecture akin to GPT-3. This model, published April 4th, 2022, achieves breakthrough capabilities on language understanding and generation, reasoning, and coding tasks. For example for reasoning tasks, PaLM used chain-of-thought prompting, which applies simulated inner monologue to solve grade school level math questions. In this talk, Vaclav will discuss both general-public-accessible intuition of ituition of how knowledge and reasoning can be represented in computers as well as technical details of PaLM model architecture.
	