---
title: Symbolic vs Connectionist Machine Learning
description: Symbolic, neural network based, and hybrid systems.
categories: ml
date: 2023-03-24
last_modified_at: 2023-01-24
image: /images/fill-versions-from-python-environment.png
layout: post
permalink: /:categories/:title
---

And today I would like to tell you what is increasingly becoming popular in the large language models. What I think will be a future of this field that could potentially provide some things I think are missing for us perhaps to get to the artificial general intelligence. As defined, maybe as a system that can do as many things as, for example, an average person in 2022.

And, okay, so what are the missing things in the current system? So now we burn through a gajillion, it's like trillions of floating point operations with all these multiplications and we still get hallucinations and we still get quite poor reasoning capabilities. And there are approaches reducing this, but something still seems to be missing. And yes, it's true that humans as well hallucinate. For example, you may have a false memory, so maybe you don't remember everything perfectly. For example, if there is a crime, oftentimes the people have actually different accounts of the death of the victim.

But we already developed approaches how to deal with these issues. We write down symbols and into the symbols we can even encode rules. And the rules can operate on the symbols and it's like a perfect system. It was super successful in understanding the world because we used it in mathematics, we used it to do accounting. And as well, because we can write things down and they don't disappear, they stay the same. It's an immutable storage. We can also think about the things multiple times. And another thing that perhaps is also important is that we can act in the world and so we can sample new data and learn new things from experience.

And these successful approaches were then later used to develop computer programs which are also rules, which operate also on some symbols. So the same way we actually built these computers which take something that's crystal perfect and it can produce something that's still crystal perfect. And while all at the same time this seems to be missing in the language model, this sort of aspect is not quite there.

And so again, what are the differences between the systems that we have? Now we have presentations that talk about what we have now. It has great capabilities, we can use it for many things, but it has also issues. And so if we can compare what I'm talking about, these symbolic approaches, that use these symbols and rules, with what we have, which is like dominant results in current times. So on one hand we have these symbols and rules, and on the other hand we almost have something like feelings and intuitions. Something that is able to find patterns in huge amounts of data. And on the other hand, it's something where we can put in, after a lot of work, we can put in some great rule set that will work. And it will work so well that it doesn't matter how big is the input.

In maths, you can take equations and you can input an x and the x can go to infinity. And the equation will still work. It will still be great. While in the neural networks, this is not quite possible. You have always some kind of rotation. So always you are somewhere between minus one and one. And so you can't really do something like this. So as I mentioned here now, these large language models are neural networks. So these neural networks just take some floating point numbers, and they do some matrix operations, maybe a couple of extra operations on top of that. And they produce vectors, like these arrays of numbers. And that's what is the inner representation of the model. That's sort of what it feels. And if it changes just a little bit, then it repeats what it thinks about. It's a very similar thing. While on the other hand, in the computations, we have the hard symbols. And that's a very different thing.

Also, in the symbolic domain, we have these, instead of neural networks and gradient boosting, on the side of the symbolic approaches, we have these decision trees. For example, you can see one here. So where you operate only on very interpretable inputs that come in at the beginning. And so those are super understandable. They are simple. And they have different capabilities in different types of situations.

And so these algorithms, these computer programs, these rules, were actually used to be, it was a dominant approach in machine learning, in artificial intelligence in 1980s. And it seemed like if this is going to work, if we are just, you know, AGI is just around the corner, right? We are going to have it, like, tomorrow. And suddenly there started to be issues. It wasn't progressing anymore. And so what these systems were doing, what was the approach back then? And why it collapsed?

So back then, the approach was that you would have even, like, dedicated hardware, like the one on the right, and you would write for your problem. You would collect a group of experts that would understand the domain. You would also have the programmers that would be able to actually write the rules. And over some time, you would build up the system, and it would actually work in some case. But it was expensive. It was very hard to update. If something changed, it was kind of a brittle.

And also in some domains, if you think about it, the experts don't actually really know what they are doing. They don't really know why they are doing it in this case this way. There is sort of this tacit knowledge, which is more, sometimes maybe like beyond words, or you would have to construct some new words, and maybe you would have to have something soft in between the words, between the symbols. And so while everything seemed great, the solutions were not able to get to the AGI. The dedicated hardware companies got into big issues where the hardware changed because there were new, more general systems. And also the different sort of approach that was leveraging neural networks, relying on more general hardware, more pop hardware, with more data, was starting to get traction. And so this was mostly an abandoned approach.

And then here came the neural networks. And there, researchers Hinton, Lecun, Bengio, led the neural network revolution in 2010. And this approach became so pervasive that, for example, people were saying, deep learning is just going to solve everything. This is it. We don't need symbolics. This is going to just solve everything. And so people started to use neural networks, they were using supervised learning where you have labels, so you know what is the target. And in that case, there was this breakthrough in development of AlexNet, where suddenly it was possible to classify images with very good accuracy on this popular dataset, which, you know, before, how would you actually describe rules for images? Like how would you recognize what's in the image by using rules? It seems rather difficult. And it is. It was possible, it would have been not. But it's very difficult and very brittle. Then over time, there were also studies of language, and people started to build these statistical models of representing words as these vectors, as this array of floating point numbers.

And for example, in 2013, there was a paper published in which actually one Czech researcher, whose name is Mikolov, participated in the research in a major way. And so this was an approach where you represented each word as one vector. Then over time, in 2017, transformers, where you could understand the entire context, where you input multiple words, you could actually understand sort of where you are in the space, not just in case for each word individually, but suddenly you were in some thought space that had many more dimensions, and it was possible to get good results with these. And people started predicting next word in web-scale datasets. We started getting interesting results.

And now we are just in the situation where we also implemented this reinforcement learning from human feedback, where we started hiring experts, and we are starting to get dedicated hardware, and suddenly it sounds... And AGI is just behind the corner, right? It's just there. I mean, there may be some similarities. Is this really the final approach, or is this the final answer, or do we need to add something more?

And so regarding this reinforcement learning from human feedback, that's actually a very interesting approach. It's not quite the same as before. Now the experts don't have to actually write some super complicated thing. They can read you a text. They can say, okay, this text is better than this text. And their result is not too small. For one case, they actually get a bigger lever, and they have a much bigger impact. So this is a much more cost-effective approach. So it's not the same situation. We are in a much better situation, of course. And as well, I spoke about the hallucinations. They are also improved with this reinforcement learning from human feedback. But we will see that you can get much more, like these other approaches of using symbols and being able to actually...

These very scalable symbolic approaches of the algorithms and code that's so useful, and it seems to be so missing. It's definitely going to be applied. So what I think is necessary and what's increasingly being applied is hybridizing both of these approaches. So using both neural networks and symbolic approaches at the same time.

The language models already use the symbols. They use symbols as their input and as their output. So they are a combination a little bit in this sense. But you can go much further. We can, because the inputs are symbols and outputs are symbols, we can plug in the systems, we can plug in the algorithms.

So for example, one of the simplest ways where you can start doing some sort of thinking inside of the language models is when you guide it with a prompt. It always tries to tell you the next word, then go to the next word. So you can guide it by writing it to the prompt. So before deciding this, let's think step by step. And with this, you will actually get improvement immediately. So you are already giving it this multistep process where it can iterate in some algorithmic way and you will get better results.

Also, another approach is this self-consistency. So you combine again this language model with the approach where you request multiple results. In the paper AlphaCode, they produce many code samples and then they look, they sample from such samples that cluster together. They are very similar. And from the places where there are lots of similar samples, those actually, it works great if you sample from the biggest one first the most.

And another approach, released very recently, it's called Toolformer. So this, again we know that the language model can take in, sees the past, and it can predict the next token. So what if it learned to exactly call something outside, but not in some template way. Of course you can do the templates, you can ask questions and then you wait. Like what kind of output can be here? It just gives me some kind of name and that's the right output. But instead here you can have it freely learn to ask at the right point. So for example, if there is a question for some sort of calculation, and it sort of writes down some continuation, some text, and you can put a lot of small training sets, you can teach it to call, calculate the function, which it can use to do the calculation for it. Maybe it's not so good in doing the calculations, and it can learn to call something external. It will get the results as well into the context. So instead of predicting the output, it would get the output from the model, it would sort of stop predicting for a couple tokens and get the result. And using this you can improve actual performance on dedicated tasks for, you can use a calculator, that can do some question answering for you, to speed it up and so on.

AlphaZero, where you have this chess playing engine, which gets superhuman. How does it do that? So it does learn from playing against itself. That's a very important part. But also the space of all the possible actions in chess. It's too big. It cannot go through everything. It has to go somewhere where it's interesting. How can you do that? You use neural guided search, where the network serves as an intuition. And it can sort of think about, okay, now I'm in this position, and I think I could do this. This sounds good, I could play this. And then it can, with self play, sort of go further into the direction, it can evaluate and learn, using this approach very effectively, and it becomes superhuman.

Then there is this retro transformer. So that one, instead of just having everything stored in its parameters, it actually stores as well the database of the training set, and it can retrieve on demand everything it saw during training. And of course, you know, it can generate queries on the internet and can give you the right summary of what you are asking.

And now I wanted to mention Coder. So this is a super intuitive idea of if you wanted to solve some complicated coding problem, how do you do about it? You usually use lots of libraries, and how do those libraries work really? So this approach actually learns to build its own function library. So in first iteration, it solves a couple of simpler coding problems. Then it stores the programs it found, it sort of analyzes them very quickly using algorithms. It then finds what are the most interesting functions to remember, and it stores them in the library. And once it has new functions, it suddenly becomes easier to solve the other problems. So this one combines the search and the memory of building this library to solve coding problems.

And you also may be asking about how do you integrate images, and you can do that as well. You have the image can be broken down into patches, you can detect things in the patches, and there is this paper, this paper, ViperGPT, which can reason on images by, you know, it gets questions, it generates codes that can answer the question on the images, and then it does sort of a course on the image using this code. So it can count how many muffins there are in the image. So it synthesizes code, which then calls detection of muffins, and then it just sums how many of them are. So if it can detect the muffins, then the summation is like simple, like it's mathematics is like super simple. I have the code, like it's a couple instructions, it's not trillions of matrix multiplications.

And for example, Jan Lecun was big on connectionist approaches now, recently published Augmented Language Model, which is an interesting survey of these approaches. You can have a look at it as well. And also, if you wanted to really, people are also asking, like how exactly is the GPT bad? Can we find some set of problems on which the GPT completely fails while humans do great? But if you want AGI, you want to be looking for something like this. You don't want to be having hundreds tasks. That's not interesting. If you go to AGI, you have to have something like this. And Francois Chollet developed a dataset on which GPT-3 got zero. I think GPT-4 is not evaluated yet. They are developing further models, and we can read about that as well.

OpenAI are very good at listening. What are the problems with their system? And right now, what seems to be their view is that this reinforcement learning from human feedback will solve most of the problems and will actually get them to AGI. So they have smart researches. They know about this, but they think providing these expert answers and comparing will get them to AGI. But at the same time, they cannot scale it infinitely, because this is perhaps not enough. Because this reinforcement learning from human feedback equates to expert labelling.

The cost for the labelling is still not as costly as model training right now, but it's getting harder. As the model gets better at the tasks, it gets harder to evaluate the results. And so now they are thinking about using the AI to assist this reinforcement learning approach to help those experts to do the review. And it helps. This approach helps. It's again like iterating. It's critiquing itself. So it gets some tasks, and then the machine produces the answer and then criticizes the answer and then tries to improve the answer. And it displays it all to the evaluators, and the evaluators point out even more problems when they get this input. And so the idea would be eventually to have very minimal input, and the machine would be sort of through self-critiquing getting better and better with less input. So getting even bigger lever to the experts.

If you wanted to learn more about this, there are companies and people which publish things in this domain. For example, on Twitter, you can follow Gary Marcus, you can follow Francois Chollet, and other authors of the papers. Also, what I found interesting is that here in Prague, there is also this startup called Filuta AI, which they are even explicitly thinking about composing AI, they want to use planning combined with that, with algorithms that can detect, provide these converts from this soft input into some hard symbols, and then do planning for companies. There is this company, Replit, which does coding tools. They want to iterate with programmers to have minimal input in creating software. There is also a Equilibrium technologies, they use reinforcement learning, but I think it's a little bit related. So you can find resources using the papers, and so on.

Shortly, I think we will get to the AGI and what impact it will have on society. I'm not sure if we can get there. We are obviously improving the results, and this will have a huge impact. Can there be another AI winter? I think it's, I wouldn't dismiss that completely. It's very easy to live in the bubble that everything is working great. And what will be the impact on society? So here I think that people actually, now there are so many easy problems that they don't really, they are not that challenged, and I think people have huge abilities that they can achieve much greater things if they are required to. And so that will be all from me. Thank you very much. Thank you.


## Questions and Answers

What specifically do you use or do in life? This was theoretical, so I would like to know what's involved. This is theoretical. We do use transformer architecture, because we have vision and language problems, so we use both. There is this collaborative group where you can see what, maybe a list of examples. And we have several things that we solve. It's the recommendation. It's very different, it's pretty much, very much symbolic, I would say, because it's very different sort of problem. And it naturally is more, less neural networks and more like scalable approaches, like symbolic. And yes, we also do similarity between the different types of images, so from search, yes.

Did you find anything that didn't work? Like, did you want to apply this? Yes. Well, in the industry, it's a lot about the cost. You have to think about, is it, how fast can I develop this? How expensive is it going to be? And I think it's great. In the previous presentations, it was very much mentioned. So it usually doesn't work in many scenarios. It's a lot about the cost. Like, you can get a huge, super-performance model, but if you cannot pay for it with what it's doing, then you cannot use it. But, yeah, like, hardware is getting cheaper, so, like, it's going to be more and more, like, moving forward.
