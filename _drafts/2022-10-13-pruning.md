---
title: Neural Network Pruning
description: Neural network pruning.
layout: post
categories: ml
date: 2022-09-16
last_modified_at: 2022-09-16
permalink: /:categories/:title
---


## Why pruning?
Reduce prediction costs by modifying the model after training, compress model, and improve generalization.
Pruning **removes weak connections** from spurious correlations improving generalization, but also small correlations worsening the error.
Pruning **at first improves test accuracy (generalization)**, until a maximum is reached, after which it decreases accuracy.
Pruning can be also viewer as **lossy compression of the model** with corresponding impact on model's latency.

![decision tree cost complexity pruning improves test accuracy until a maximum, scikit docs](/images/decision_tree_cost_complexity_pruning__improves_test_accuracy_until_a_maximum__scikit_docs.png) 

## Decision Trees
Decision trees are composed of **a tree of if-else statements of the input variables**, consequently they are **interpretable**.
They **overfit without regularization** is applied by encoding entire dataset.
Commonly **regularization methods are pruning, minimum samples per leaf node, or maximum tree depth**.
Popular algorithms available:
- [C4.5](https://www.amazon.com/C4-5-Programs-Machine-Learning-Kaufmann/dp/1558602380) and its predesor [ID3](https://link.springer.com/content/pdf/10.1007/BF00116251.pdf), extension C5.0. C4.5 is one of the [top 10 algorithms listed in ML in 2008 survey paper](http://www.cs.umd.edu/~samir/498/10Algorithms-08.pdf).
- [CART](https://www.amazon.com/Classification-Regression-Wadsworth-Statistics-Probability/dp/0412048418/) another popular similar approach. Used by [Scikit-learn](https://scikit-learn.org/stable/modules/tree.html#classification).

![decision tree Iris dataset, Scikit documentation](/images/decision_tree_iris_dataset__scikit_docs.png)


## Minimal Cost-Complexity Pruning in CART Decision Trees
Minimal Cost-Complexity Pruning is **a greedy algorithm, which iteratively removes the best to prune subtrees** until reaching a specified limit.

For each non-terminal node `t` and we can calculate cost complexity of its subtree:
`def cost_complexity(t): misclassification_rate(t) + alpha * n_terminal_nodes(t)`

In simple terms, we start with `alpha_j` of `0` and increase it until we find a node, for which `cost_complexity(t)` would be lower if pruned, and we prune this node next.
We repeat this until we each a specified limit `alpha`.

![decision tree prunining (Kijsirikul 2001)](/images/decision_tree_pruning__kijsirikul_2001.png)


## Decision Tree vs Neural Network
- Vanilla **decision trees split on the input variables only**, while neural networks also train linear transformation of the values.
- But [neural networks with piecewise activation functions (e.g. ReLU) are equivalent to extension of a decision tree](https://arxiv.org/pdf/2210.05189.pdf) with hyperplane decision boundaries called [Multivariate Decision Trees](https://link.springer.com/content/pdf/10.1023/A:1022607123649.pdf).

![neural network as a multivariate decision tree for a parabola dataset](/images/neural_networks_are_decision_trees__aytekin_2022.png)


## Pruning in Neural Networks
We start with random initialization allowing all connections.
[Learning both Weights and Connections for Efficient Neural Networks paper](https://arxiv.org/pdf/1506.02626.pdf) prunes the smallest weights (the weakest connections) and then crucially trains the new network.
This process can repeat.
[The Lottery Ticket Hypothesis paper](https://arxiv.org/abs/1803.03635) pruning 90% its [smallest weights (the weakest connections)](https://arxiv.org/pdf/1506.02626.pdf).

[Tensorflow pruning API](https://blog.tensorflow.org/2019/05/tf-model-optimization-toolkit-pruning-API.html) sparsifies the layer's weights during training. e.g. MNIST
For example, using this with 50% sparsity will ensure that 50% of the layer's weights are zero.



![pruning synapses, neurons, layers](/images/pruning_both_synapses_and_neuron_nodes_han_2015.png)
![pruning steps](/images/pruning_steps__han_2015.png)


## Neural Magic Pruning and Execution Engine
- Running predictions on CPU with GPU speed and precision, saving up to 2-4x costs with more device availability.
- Popular models like BERT and Resnet50 in vision and text available.
- They combine known techniques of pruning, quantization, and distillation in a compounding way to run fast smaller models on CPU. On CPU get speedups of 100x.
- Have to use their closed source executor to efficiently utilize CPU cores.


Citing from [A Software Architecture for the Future of ML - Neural Magic](https://neuralmagic.com/technology/):
- But once FLOPs are reduced, the computation becomes more “memory bound”, that is, there is less compute per data item, so the cost of data movement in and out of memory becomes crucially important. ...
- Neural Magic solves this on commodity CPU hardware by radically changing the neural network software execution ... rather than execute the network layer after layer, we are able to **execute the neural network in depth-wise stripes we call Tensor Columns**. ... Each Tensor Column stays completely in the CPU’s large fast caches along the full execution length of the neural network, ... In this way, we almost completely avoid moving data in and out of memory.
 

### People and Company
- Nir Shavit:
	- professor of computer science
	- focused on parallel computation multi-threading
- won Dijkstra price, Godel price

### Pruning
Pruning helps only with **prediction and not training**.
Training stays dense plus there is additional step of pruning.
**Fine-tuning on pruned models possible**, but requires distillation to preserve the accuracy.

### Research
[ML Performance Research Papers - Neural Magic](https://neuralmagic.com/resources/technical-papers/)
e.g. [https://arxiv.org/pdf/2203.07259.pdf](https://arxiv.org/pdf/2203.07259.pdf)
- [oBERT: GPU-Level Latency on CPUs with 10x Smaller Models](https://neuralmagic.com/blog/obert/)
- claim 14x vs TinyBERT
- on bigger models their performance advantage goes down


### Costs
Results 4x savings on AWS Ohio, but that is optimistic. Conservatively I guess 2x.
- Instance name On-Demand hourly rate vCPU Memory Storage Network performance
- g4dn.xlarge	$0.526	4	16 GiB	125 GB NVMe SSD	Up to 25 Gigabit
- t4g.xlarge	$0.1344	4	16 GiB	EBS Only	Up to 5 Gigabit


### Experiments
- I did observe fast speed. In outperforming Distilbert latency 2.3x on my CPU with 12 cores.
- TinyBERT?
- Cuda AI Template


### Licence and Patents
Patented inference technology: [Patents Assigned to Neuralmagic Inc. - Justia Patents Search](https://patents.justia.com/assignee/neuralmagic-inc)
> to execute in parallel involve computations from multiple layers

Restrictive licence for production use of the sparse inference engine:
> solely for evaluation  use in development and testing environments, and not for production use.


### Alternatives
- [Vertigo Applied Intelligence - Vertigo.ai](https://vertigo.ai/) ?

### Summary
- vendor lock in - probably want to avoid and be ready to switch.
- no clear alternative now to me know, not sure what patent means in practice.
- but could save 2x money if there was no extra cost for the NeuralMagic by infering on CPU with similar time cost as on GPU. 
	- the next best thing would be just use similar methods without their specific technology
	- How much they charge?
- still need to train on GPU