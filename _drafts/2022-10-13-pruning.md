---
title: Neural Network Pruning
description: Neural network pruning.
layout: post
categories: ml
date: 2022-09-16
last_modified_at: 2022-09-16
permalink: /:categories/:title
---


## Why pruning?
Reduce prediction costs by modifying the model after training.
Have to use their closed source executor.
They combine known techniques of pruning, quantization, and distillation in a compounding way to run fast smaller models on CPU. On CPU get speedups of 100x.
Pruning can also reduce model disk size.


## Decision Tree Pruning
C4.5 decision tree from 1993 (C4.5: Programs for machine learning.) uses pruning. C4.5 is one of the [top 10 algorithms listed in ML in 2008 survey paper](http://www.cs.umd.edu/~samir/498/10Algorithms-08.pdf).
1. Overfits a decision tree on the train set.
2. Estimates the error of a change and prunes starting from the leafs moving towards the root again using the samples from train set:
   - replace subtree with a leaf 
   - replace a subtree with a branch


![decision tree prunining (Kijsirikul 2001)](/images/decision_tree_pruning__kijsirikul_2001.png)


## Pruning in Neural Networks
[Learning both Weights and Connections for Efficient Neural Networks paper](https://arxiv.org/pdf/1506.02626.pdf) prunes the smallest weights (the weakest connections) and then crucially retrains rest of the network.
This process can repeat.
[The Lottery Ticket Hypothesis paper](https://arxiv.org/abs/1803.03635) pruning 90% its [smallest weights (the weakest connections)](https://arxiv.org/pdf/1506.02626.pdf).

[Tensorflow pruning API](https://blog.tensorflow.org/2019/05/tf-model-optimization-toolkit-pruning-API.html) sparsifies the layer's weights during training. e.g. MNIST
For example, using this with 50% sparsity will ensure that 50% of the layer's weights are zero.



![pruning synapses, neurons, layers](/images/pruning_both_synapses_and_neuron_nodes_han_2015.png)


## Neural Magic Pruning and Execution Engine
Running predictions on CPU with GPU speed and precision, saving up to 2-4x costs with more device availability.
Popular models like BERT and Resnet50 in vision and text available. 


Citing from [A Software Architecture for the Future of ML - Neural Magic](https://neuralmagic.com/technology/):
- But once FLOPs are reduced, the computation becomes more “memory bound”, that is, there is less compute per data item, so the cost of data movement in and out of memory becomes crucially important. ...
- Neural Magic solves this on commodity CPU hardware by radically changing the neural network software execution ... rather than execute the network layer after layer, we are able to **execute the neural network in depth-wise stripes we call Tensor Columns**. ... Each Tensor Column stays completely in the CPU’s large fast caches along the full execution length of the neural network, ... In this way, we almost completely avoid moving data in and out of memory.
 

### People and Company
- Nir Shavit:
	- professor of computer science
	- focused on parallel computation multi-threading
- won Dijkstra price, Godel price

### Pruning
Pruning helps only with **prediction and not training**.
Training stays dense plus there is additional step of pruning.
**Fine-tuning on pruned models possible**, but requires distillation to preserve the accuracy.

### Research
[ML Performance Research Papers - Neural Magic](https://neuralmagic.com/resources/technical-papers/)
e.g. [https://arxiv.org/pdf/2203.07259.pdf](https://arxiv.org/pdf/2203.07259.pdf)
- [oBERT: GPU-Level Latency on CPUs with 10x Smaller Models](https://neuralmagic.com/blog/obert/)
- claim 14x vs TinyBERT
- on bigger models their performance advantage goes down


### Costs
Results 4x savings on AWS Ohio, but that is optimistic. Conservatively I guess 2x.
- Instance name On-Demand hourly rate vCPU Memory Storage Network performance
- g4dn.xlarge	$0.526	4	16 GiB	125 GB NVMe SSD	Up to 25 Gigabit
- t4g.xlarge	$0.1344	4	16 GiB	EBS Only	Up to 5 Gigabit


### Experiments
- I did observe fast speed. In outperforming Distilbert latency 2.3x on my CPU with 12 cores.
- TinyBERT
- Cuda AI Template


### Licence and Patents
Patented inference technology: [Patents Assigned to Neuralmagic Inc. - Justia Patents Search](https://patents.justia.com/assignee/neuralmagic-inc)
> to execute in parallel involve computations from multiple layers

Restrictive licence for production use of the sparse inference engine:
> solely for evaluation  use in development and testing environments, and not for production use.


### Alternatives
- [Vertigo Applied Intelligence - Vertigo.ai](https://vertigo.ai/) ?

### Summary
- vendor lock in - probably want to avoid and be ready to switch.
- no clear alternative now to me know, not sure what patent means in practice.
- but could save 2x money if there was no extra cost for the NeuralMagic by infering on CPU with similar time cost as on GPU. 
	- the next best thing would be just use similar methods without their specific technology
	- How much they charge?
- still need to train on GPU