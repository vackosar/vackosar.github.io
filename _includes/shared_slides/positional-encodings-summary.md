- [self-attention](/ml/transformers-self-attention-mechanism-simplified) and [feed-forward layers](/ml/Feed-Forward-Self-Attendion-Key-Value-Memory) are symmetrical with respect to the input
- so we have to provide positional information about each input token
- so [positional encodings or embeddings](/ml/transformer-positional-encodings) are added to [token embeddings](/ml/transformer-embeddings-and-tokenization) in [transformer](/ml/transformers-self-attention-mechanism-simplified)
- encodings are manually (human) selected, while embeddings are learned (trained)

