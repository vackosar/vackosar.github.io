 
- [Tokenization](/ml/Tokenization-in-Machine-Learning-Explained) is cutting input data into meaningful parts that can be [embedded](/ml/Embeddings-in-Machine-Learning-Explained) into a vector space.
- image is split into patches, text is split into tokens (frequent words) e.g. [transformer tokenization](/ml/transformer-embeddings-and-tokenization)
- Sometimes we also add [token position is added to their embeddings](/ml/transformer-positional-embeddings-and-encodings).

