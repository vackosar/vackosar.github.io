---
title: Synthetic Data for LLM Training
description: Learn more about using generated training data in large language model training.
categories: ml
date: 2024-02-20
last_modified_at: 2024-02-28
layout: post
permalink: /:categories/:title
image: /images/robot-writing-synthetic-data-thumb.webp
my_related_post_paths:
- _posts/2023-03-24-Symbolic-vs-Connectionist-Machine-Learning.md
- _posts/2022-04-18-Understand-Large-Language-Models-like-ChatGPT.md
- _posts/2023-06-08-Prompting-Techniques-That-Sqeeze-The-Best-Out-of-Your-LLM.md
- _posts/2023-07-04-How-to-Create-a-Machine-Learning-Dataset.md
- _posts/2022-04-10-googles-pathways-language-model-and-chain-of-thought.md
- _posts/2022-09-11-Embeddings-in-Machine-Learning-Explained.md
- _posts/2021-04-27-dreamcoder-ai-wake-sleep-program-learning.md
---

![Robot writting synthetic data for its children :D](/images/robot-writing-synthetic-data-thumb.webp)

Here are my notes on synthetic data. Take it with a grain of salt, as I am new in this area.

Let's say you have a generative AI (generative machine learning) problem, so you have some input data and some corresponding output data. But you have only a tiny quantity of this training data.

Synthetic data is data generated by a machine learning model. Here we will discuss primarily large language models (LLMs). Synthetic data helps where you need to add missing hard-to-collect data for your predictions. This data could be something people never write down or say. For example, human problem-solving thoughts usually don't get written down. [Let's Verify Step by Step](https://arxiv.org/abs/2305.20050) uses synthetic data to create PRM800K dataset for math step-wise reasoning dataset not available elsewhere:

For this kind of PRM800K dataset is rare and with additional each step ranking it is even rarer:

```
Let's call the numerator x.

So the denominator is 3x-7.

We know that x/(3x-7) = 2/5.

So 5x = 2(3x-7).

5x = 6x-14.

So x = 7.
```


The rarer and more critical the data, the more impactful the synthetic data can be.

If nothing like the required data was present during the model pretraining, you won't be able to prompt-instruct the model to perform the required action. Few shot examples can help, but the more complex the problem, the more likely you will need more examples, which are costly to write by hand.

Another advantage of synthetic data is that they can be cleaner than data collected from random places on the internet. The disadvantage is that they can be less diverse, less representative than the real data, and contain more hallucinations.


## Why synthetic data makes sense?
Real data costs human time, and synthetic data can be a way around that. But you cannot create the required data synthetically out of thin air.

You may need:
1. Either a more general model that was essentially trained on something similar to the target data needed,
2. Or you need some real data that is close to the required data and perform only an easy modification to match the required data distribution.

Another way of looking at 2 is to use other data, which was trained into LLM, to shift the output distribution the way you want. For example, polishing the LLM behavior by making it consistent with selected good patterns in some parts of the training data, which are retrieved and applied with prompting instructions, leads to generating good synthetic data.


## Levels of Synthetic Data

Here is a spectrum of increasingly less human involvement in the process or human leverage in the process or model development.

1. Fully manual: You would love to train on data from people that is fully manually written and verified.
2. Cleaned-up manual data: The data is manually written but rewritten, rephrased, or cleaned by a machine, then verified.
3. The data is entirely generated but manually verified, and each sample is labeled.
4. The data is entirely generated and rated by a machine. The trained model is evaluated on a small human-labeled subset.
5. Self-aligning or self-polishing: The data is entirely generated and rated by a machine. The trained model is evaluated on machine-generated data.
6. Autonomous: Self-improving from the environment and from generated data. The [Q-transformer](/ml/Bellman-Update-and-Synthetic-Data-in-Q-Transformer) is a step towards that.


## Verification of Synthetic Data
Garbage-in implies garbage-out.
The more complex the data to generate or the more distant to the real training data, the harder it is to synthesize the correct data to train on.

In some problems, verification is easier than generation, so you can remove the invalid data from the generated data. For example, the goal may be to generate a program function that passes an executable set of tests. In this case, verifying that the generated sum is correct is very easy.

[Constitutional AI](https://arxiv.org/abs/2212.08073) uses an LLM to label synthetically generated responses to follow specific rules (constitution about harm, bias, and more).


## Augmenting or Expanding Human Data
[WizardLM Evol Instruct](https://github.com/nlpxucan/WizardLM) uses human-written coding examples to generate more difficult (complex) examples with GPT-3.5. Then, they use the synthetic dataset to fine-tune and improve performance on coding problems in general.


## Inverted Generation of Instructions from Outputs
In some cases, generating the questions (inputs) is easy given the answers (outputs). This inverted generation also allows you to control the distribution of the labeled samples. In the case of text classification, it is easier to generate a text that matches the given category label.
Another example of this method is [Self-Alignment with Instruction Backtranslation](https://arxiv.org/abs/2308.06259).


## Examples of Synthetic Data Applications
What synthetic data tools can you use today?

### [Teknium's Nous-Hermes-2-Mistral-7B-DPO](https://huggingface.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO)

Mistral 7B fine-tunes on open Hermes synthetic datasets are one of the best OSS models in this weight count category. 

Nous-Hermes-2-Mistral-7B-DPO was trained on:
- Supervised fine-tuning synthetic dataset teknium/OpenHermes-2.5 generated by GPT-4 in size of 1,000,000 instructions/chats.
- Direct preference optimization dataset, likely also GPT-4 generated (synthetic).


### [DSPy Python Library](https://github.com/stanfordnlp/dspy?tab=readme-ov-file) 
DSPy can take just tens of labeled examples and high-level LLM chains and generate prompts to use, generate synthetic data, and fine-tune a smaller model.

This library generally helps you build prompt chains or pipelines where the LLMs have well-defined inputs and outputs and various tools like RAG. 

The library abstracts away manual prompt engineering and instead optimizes the prompts for you, such that you only focus on structured and documented inputs and outputs.
DSPy seems much more practical than LangChain.

The library uses a selected larger model (GPT-3.5 or Llama2 13b) to generate prompts and few-shot examples for your smaller LLM like T5. Not only that, you can compose an entire pipeline or prompt chain. Generating and optimizing the prompts within the prompt chain is called compiling.
For example, DSPy can generate reasoning examples and optionally fine-tune a smaller LLM on them.
The question is, how good are the examples?
Un-compiled chains use zero-shot prompting.

Here is an example DSPy project in [a video](https://www.youtube.com/watch?v=41EfOY0Ldkc). 


## Research Papers Relevant to Synthetic Data
Below is a partial list, and I may expand on this in the future.

### [Weak-to-Strong Generalization: Eliciting Strong Capabilities With Weak Supervision](https://arxiv.org/html/2312.09390v1)
The smaller model teacher model generates inputs and labels.
The bigger model can learn to outperform a weaker teacher if allowed to be "over-confident."
However, this approach is not generally proven for all situations and still shows an upper-performance limit.

### [Let's Verify Step by Step](https://arxiv.org/abs/2305.20050)
A math reasoning dataset.
Contributions of this paper are: 

1. process supervision can train more reliable reward models than outcome supervision. State-of-the-art process-supervised reward model solves 78.2% of problems from a representative subset of the MATH test set.
2. A large reward model can reliably approximate human supervision for smaller reward models and can efficiently conduct large-scale data collection ablations.
3. active learning leads to a 2.6Ã— improvement in the data efficiency of process supervision.
4. full process supervision dataset, PRM800K, a dataset of 800K step-level labels across 75K solutions to 12K problems). Assign each step in the solution a positive, negative, or neutral label.
